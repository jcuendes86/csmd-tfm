steps:
  # 1) Ejecucion del trabajo en Dataflow Flex Template
  - name: gcr.io/google.com/cloudsdktool/cloud-sdk
    id: Run Dataflow Flex Template Job
    args:
      - gcloud
      - dataflow
      - flex-template
      - run
      - "cars-dataset-pipeline-job-${SHORT_SHA}"
      - "--project=${PROJECT_ID}"
      - "--region=${_REGION}"
      - "--template-file-gcs-location=${_TEMPLATE_BUCKET_NAME}"
      - "--parameters=input=${_DATASET_BUCKET_NAME},output_table=${PROJECT_ID}:${_BQ_DATASET}.${_BQ_TABLE}"
      - "--staging-location=${_STAGING_BUCKET_NAME}"
      - "--temp-location=${_TEMP_BUCKET_NAME}"
      - "--service-account-email=${_DATAFLOW_WORKER_SA_EMAIL}"

options:
  logging: CLOUD_LOGGING_ONLY

substitutions:
  # Región de Artifact Registry (p.ej. europe-southwest1, europe-west1, etc.)
  _REGION: 'europe-west1'
  # Ubicación GCS de la plantilla Flex (DEBE empezar por gs://)
  _TEMPLATE_BUCKET_NAME: 'gs://MI_BUCKET/templates/cars_dataset_pipeline.json'
  # Ubicacion del dataset (DEBE empezar por gs://)
  _DATASET_BUCKET_NAME: 'gs://MI_BUCKET/coches-segunda-mano-test.csv'
  # Nombre del bucket de staging (DEBE empezar por gs://)
  _STAGING_BUCKET_NAME: 'gs://MI_BUCKET/staging'
  # Nombre del bucket de temp (DEBE empezar por gs://)
  _TEMP_BUCKET_NAME: 'gs://MI_BUCKET/temp'
  # Nombre del dataset en BigQuery
  _BQ_DATASET: 'defaultDataset'
  # Nombre de la tabla en BigQuery
  _BQ_TABLE: 'defaultTable'
  # Email del Service Account de Dataflow Worker
  _DATAFLOW_WORKER_SA_EMAIL: 'dataflow-worker-sa@my-project.iam.gserviceaccount.com'
